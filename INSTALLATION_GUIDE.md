# ğŸ› ï¸ Jetson AGX Orin DeepResearch å®Œæ•´å®‰è£…æŒ‡å—

## ğŸ“‹ å®‰è£…æ­¥éª¤æ€»ç»“

### ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒå‡†å¤‡ï¼ˆå…³é”®æ­¥éª¤ï¼‰

#### 1. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥
```bash
# æ£€æŸ¥Jetsonç¡¬ä»¶å’Œç³»ç»Ÿç‰ˆæœ¬
uname -a                          # ç¡®è®¤Linux 5.15.148-tegra
nvidia-smi                        # éªŒè¯GPUé©±åŠ¨å’ŒCUDA 12.6
free -h                           # æ£€æŸ¥å†…å­˜ï¼ˆåº”æœ‰64GBï¼‰
df -h                             # æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆæ¨¡å‹éœ€è¦20GB+ï¼‰
```

#### 2. å®‰è£…å¿…è¦å·¥å…·
```bash
# å®‰è£…åŸºç¡€å·¥å…·
sudo apt update && sudo apt install -y \
    git wget curl vim htop tree \
    build-essential cmake \
    python3-pip python3-dev

# å®‰è£…Dockerï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
curl -fsSL https://get.docker.com | sudo sh
sudo usermod -aG docker $USER
```

#### 3. é…ç½®Huggingfaceè®¿é—®
```bash
# å®‰è£…huggingface-cli
pip install --upgrade huggingface_hub

# ç™»å½•ï¼ˆå¿…éœ€æ­¥éª¤ï¼‰
huggingface-cli login
# è¾“å…¥ä½ çš„access token
```

### ç¬¬äºŒé˜¶æ®µï¼šè·å–llama.cppï¼ˆå®¹å™¨æ–¹æ¡ˆï¼‰

#### æ–¹æ¡ˆAï¼šä½¿ç”¨ç°æœ‰å®¹å™¨ï¼ˆæ¨èï¼‰
```bash
# æ£€æŸ¥ç°æœ‰å®¹å™¨
docker images | grep jetson
docker ps -a | grep jetson

# å¦‚æœæœ‰ç°æˆçš„PyTorch+CUDAå®¹å™¨ï¼Œç›´æ¥ä½¿ç”¨
# ä¾‹å¦‚ï¼šjetson_minicpm_v:latest
```

#### æ–¹æ¡ˆBï¼šæ‰‹åŠ¨ç¼–è¯‘llama.cpp
```bash
# å…‹éš†llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# ç¼–è¯‘CUDAç‰ˆæœ¬
make -j$(nproc) LLAMA_CUDA=1

# éªŒè¯ç¼–è¯‘ç»“æœ
ls -la build/bin/llama-cli build/bin/llama-server
```

### ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å‹ä¸‹è½½ï¼ˆå…³é”®ï¼‰

#### 1. åˆ›å»ºæ¨¡å‹ç›®å½•
```bash
mkdir -p /data/sensor-voice/models
cd /data/sensor-voice/models
```

#### 2. ä¸‹è½½Q4_K_Mæ¨¡å‹ï¼ˆ18GBï¼‰
```bash
# ä½¿ç”¨huggingface-cliä¸‹è½½ï¼ˆå·²ç™»å½•çŠ¶æ€ä¸‹ï¼‰
huggingface-cli download \
  bartowski/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-GGUF \
  Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf \
  --local-dir .

# éªŒè¯ä¸‹è½½
ls -lh Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf
```

#### 3. å¤‡é€‰ä¸‹è½½æ–¹æ¡ˆ
```bash
# å¦‚æœä¸»é“¾æ¥å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æº
# æ–¹æ¡ˆ1: å…¶ä»–GGUFä»“åº“
huggingface-cli download \
  gabriellarson/Tongyi-DeepResearch-30B-A3B-GGUF \
  tongyi-deepresearch-30b-a3b.Q4_K_M.gguf \
  --local-dir .

# æ–¹æ¡ˆ2: ä½¿ç”¨git-lfsï¼ˆéœ€è¦å®‰è£…git-lfsï¼‰
git lfs install
git clone https://huggingface.co/bartowski/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-GGUF
```

### ç¬¬å››é˜¶æ®µï¼šDockerç¯å¢ƒé…ç½®

#### 1. åˆ›å»ºdocker-compose.yml
```bash
# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p /data/deepresearch
cd /data/deepresearch

# åˆ›å»ºdocker-compose.ymlæ–‡ä»¶ï¼ˆå†…å®¹è§é™„å½•Aï¼‰
# æˆ–ä½¿ç”¨å·²æä¾›çš„æ–‡ä»¶
```

#### 2. åˆ›å»ºå¯åŠ¨è„šæœ¬
```bash
# ä¸€é”®å¯åŠ¨è„šæœ¬ï¼ˆå†…å®¹è§é™„å½•Bï¼‰
# æˆ–ä½¿ç”¨å·²æä¾›çš„start.sh
```

#### 3. æµ‹è¯•Dockerç¯å¢ƒ
```bash
# å¯åŠ¨åŸºç¡€å®¹å™¨
docker-compose up -d jetson-deepresearch

# æ£€æŸ¥çŠ¶æ€
docker-compose ps

# è¿›å…¥å®¹å™¨æµ‹è¯•
docker-compose exec jetson-deepresearch bash
```

### ç¬¬äº”é˜¶æ®µï¼šéªŒè¯æµ‹è¯•

#### 1. ç¯å¢ƒéªŒè¯
```bash
# åœ¨å®¹å™¨å†…æµ‹è¯•PyTorch CUDA
python3 -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# æµ‹è¯•llama.cpp
export LD_LIBRARY_PATH=/workspace/llama.cpp/build/bin:$LD_LIBRARY_PATH
/workspace/llama.cpp/build/bin/llama-cli --version
```

#### 2. æ¨¡å‹éªŒè¯
```bash
# å¿«é€Ÿæ¨¡å‹æµ‹è¯•
export LD_LIBRARY_PATH=/workspace/llama.cpp/build/bin:$LD_LIBRARY_PATH
/workspace/llama.cpp/build/bin/llama-cli \
  --model /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf \
  --ctx-size 512 \
  --n-gpu-layers 10 \
  --predict 10 \
  --prompt "æµ‹è¯•" \
  --no-display-prompt
```

#### 3. æœåŠ¡å™¨æµ‹è¯•
```bash
# å¯åŠ¨æœåŠ¡å™¨
/workspace/llama.cpp/build/bin/llama-server \
  --model /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf \
  --ctx-size 8192 \
  --n-gpu-layers 35 \
  --port 8080 \
  --host 0.0.0.0

# æµ‹è¯•API
curl http://localhost:8080/v1/models
```

## âš ï¸ å…³é”®æ³¨æ„äº‹é¡¹

### 1. ç¡¬ä»¶è¦æ±‚æ£€æŸ¥
- **å†…å­˜**: å¿…é¡»â‰¥32GBï¼Œæ¨è64GBï¼ˆAGX Orinï¼‰
- **å­˜å‚¨**: è‡³å°‘50GBå¯ç”¨ç©ºé—´ï¼ˆæ¨¡å‹18GB + ç¼“å­˜ï¼‰
- **GPU**: å¿…é¡»æ”¯æŒCUDAï¼Œè®¡ç®—èƒ½åŠ›â‰¥7.0

### 2. ç½‘ç»œé…ç½®
- **Huggingfaceè®¿é—®**: éœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥
- **ä»£ç†è®¾ç½®**: å¦‚éœ€è¦ï¼Œé…ç½®HTTP_PROXY/HTTPS_PROXY
- **DNS**: ç¡®ä¿èƒ½è§£æhuggingface.co

### 3. æƒé™é—®é¢˜
```bash
# Dockeræƒé™
sudo usermod -aG docker $USER
# é‡æ–°ç™»å½•ç”Ÿæ•ˆ

# æ–‡ä»¶æƒé™
sudo chown -R $USER:$USER /data/sensor-voice
```

### 4. å†…å­˜ä¼˜åŒ–
```bash
# å…³é—­ä¸å¿…è¦çš„æœåŠ¡
sudo systemctl stop gdm3  # å…³é—­GUIï¼ˆå¯é€‰ï¼‰
sudo systemctl set-default multi-user.target

# å¢åŠ äº¤æ¢ç©ºé—´ï¼ˆå¦‚æœéœ€è¦ï¼‰
sudo fallocate -l 16G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### 5. æ€§èƒ½è°ƒä¼˜
```bash
# GPUå†…å­˜åˆ†é…
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# llama.cppä¼˜åŒ–
export GGML_CUDA=1
export LLAMA_CUDA=1

# Jetsonç‰¹å®šä¼˜åŒ–
export CUDA_ARCH_LIST=8.7
```

## ğŸš¨ å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### é”™è¯¯1: HuggingFaceè®¤è¯å¤±è´¥
```
401 Unauthorized
```
**è§£å†³**:
```bash
# é‡æ–°ç™»å½•
huggingface-cli logout
huggingface-cli login
# ç¡®ä¿tokenæœ‰æ•ˆ
```

### é”™è¯¯2: CUDAä¸å¯ç”¨
```
CUDA available: False
```
**è§£å†³**:
```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi
# æ£€æŸ¥å®¹å™¨è¿è¡Œæ—¶
docker info | grep -i nvidia
# ç¡®ä¿ä½¿ç”¨nvidia-runtime
```

### é”™è¯¯3: æ¨¡å‹æ–‡ä»¶æŸå
```
failed to load model
```
**è§£å†³**:
```bash
# é‡æ–°ä¸‹è½½
rm /data/sensor-voice/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf
huggingface-cli download ...
# éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆåº”ä¸º18GBï¼‰
ls -lh /data/sensor-voice/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf
```

### é”™è¯¯4: å†…å­˜ä¸è¶³
```
out of memory
```
**è§£å†³**:
```bash
# å‡å°‘GPUå±‚æ•°--n-gpu-layers 20
# å‡å°ä¸Šä¸‹æ–‡--ctx-size 4096
# å¢åŠ äº¤æ¢ç©ºé—´
# å…³é—­å…¶ä»–åº”ç”¨
```

### é”™è¯¯5: åº“æ–‡ä»¶ç¼ºå¤±
```
libllama.so: cannot open shared object file
```
**è§£å†³**:
```bash
export LD_LIBRARY_PATH=/path/to/llama.cpp/build/bin:$LD_LIBRARY_PATH
```

## ğŸ“Š éªŒè¯æ¸…å•

### å®‰è£…å‰æ£€æŸ¥
- [ ] ç³»ç»Ÿç‰ˆæœ¬ç¡®è®¤ï¼ˆLinux 5.15.148-tegraï¼‰
- [ ] CUDAç‰ˆæœ¬ç¡®è®¤ï¼ˆ12.6ï¼‰
- [ ] å†…å­˜æ£€æŸ¥ï¼ˆâ‰¥32GBï¼‰
- [ ] ç£ç›˜ç©ºé—´æ£€æŸ¥ï¼ˆâ‰¥50GBï¼‰
- [ ] ç½‘ç»œè¿æ¥æµ‹è¯•

### å®‰è£…è¿‡ç¨‹æ£€æŸ¥
- [ ] HuggingFaceç™»å½•æˆåŠŸ
- [ ] Dockerè¿è¡Œæ­£å¸¸
- [ ] æ¨¡å‹ä¸‹è½½å®Œæ•´ï¼ˆ18GBï¼‰
- [ ] llama.cppç¼–è¯‘æˆåŠŸ
- [ ] PyTorch CUDAå¯ç”¨

### åŠŸèƒ½æµ‹è¯•
- [ ] æ¨¡å‹åŠ è½½æµ‹è¯•é€šè¿‡
- [ ] åŸºç¡€æ¨ç†æµ‹è¯•é€šè¿‡
- [ ] APIæœåŠ¡å™¨å¯åŠ¨æˆåŠŸ
- [ ] å®¹å™¨ç¯å¢ƒç¨³å®šè¿è¡Œ

## ğŸ”§ å¿«é€Ÿæ•…éšœæ’æŸ¥è„šæœ¬

åˆ›å»º`troubleshoot.sh`:
```bash
#!/bin/bash
echo "ğŸ” Jetson DeepResearch æ•…éšœæ’æŸ¥..."
echo "1ï¸âƒ£ ç³»ç»Ÿä¿¡æ¯:"
uname -a
echo "2ï¸âƒ£ GPUçŠ¶æ€:"
nvidia-smi 2>/dev/null || echo "âŒ nvidia-smiä¸å¯ç”¨"
echo "3ï¸âƒ£ DockerçŠ¶æ€:"
docker info | grep -i nvidia
echo "4ï¸âƒ£ æ¨¡å‹æ–‡ä»¶:"
ls -lh /data/sensor-voice/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf 2>/dev/null || echo "âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨"
echo "5ï¸âƒ£ llama.cpp:"
ls -la /data/sensor-voice/sensor-voice/llama.cpp/build/bin/llama-cli 2>/dev/null || echo "âŒ llama-cliä¸å­˜åœ¨"
echo "6ï¸âƒ£ HuggingFace:"
huggingface-cli whoami 2>/dev/null || echo "âŒ æœªç™»å½•HuggingFace"
```

## ğŸ“š é™„å½•

### é™„å½•A: docker-compose.ymlæ¨¡æ¿
```yaml
version: '3.8'
services:
  jetson-deepresearch:
    image: jetson_minicpm_v:latest
    container_name: deepresearch
    runtime: nvidia
    network_mode: host
    environment:
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics
      - CUDA_VISIBLE_DEVICES=0
      - LLAMA_CUDA=1
    volumes:
      - /data/sensor-voice:/workspace
      - /data/sensor-voice/models:/workspace/models
      - /data/sensor-voice/sensor-voice/llama.cpp:/workspace/llama.cpp
    working_dir: /workspace
    command: /bin/bash
    stdin_open: true
    tty: true
```

### é™„å½•B: ä¸€é”®å¯åŠ¨è„šæœ¬
```bash
#!/bin/bash
echo "ğŸš€ å¯åŠ¨ Jetson DeepResearch..."
docker-compose up -d jetson-deepresearch
sleep 10
docker-compose ps
```

è¿™ä¸ªæŒ‡å—æ€»ç»“äº†æ‰€æœ‰æˆåŠŸçš„å®‰è£…æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹ï¼Œä½ å¯ä»¥åœ¨ä»»ä½•Jetson AGX Orinç¯å¢ƒä¸­æŒ‰ç…§è¿™ä¸ªæ ‡å‡†åŒ–æµç¨‹è¿›è¡Œéƒ¨ç½²ã€‚éœ€è¦æˆ‘è¡¥å……ä»»ä½•ç‰¹å®šçš„ç»†èŠ‚å—ï¼Ÿ