# ğŸ¯ Jetson AGX Orin DeepResearch é¡¹ç›®æ€»ç»“

## ğŸ“š æ–‡æ¡£ç´¢å¼•

### ğŸš€ å¿«é€Ÿå¼€å§‹
- **[QUICK_START.md](QUICK_START.md)** - 1åˆ†é’Ÿä¸Šæ‰‹æŒ‡å—
- **[start.sh](start.sh)** - ä¸€é”®å¯åŠ¨åŸºç¡€ç¯å¢ƒ
- **[start-server.sh](start-server.sh)** - ä¸€é”®å¯åŠ¨APIæœåŠ¡å™¨

### ğŸ“– è¯¦ç»†æ–‡æ¡£
- **[INSTALLATION_GUIDE.md](INSTALLATION_GUIDE.md)** - å®Œæ•´å®‰è£…æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹
- **[README_DOCKER.md](README_DOCKER.md)** - Dockerç¯å¢ƒè¯¦ç»†é…ç½®
- **[diagnose.sh](diagnose.sh)** - ç¯å¢ƒè¯Šæ–­è„šæœ¬

### ğŸ§ª æµ‹è¯•å’ŒéªŒè¯
- **[test_deepresearch.sh](test_deepresearch.sh)** - ç¯å¢ƒæµ‹è¯•è„šæœ¬

### âš™ï¸ é…ç½®æ–‡ä»¶
- **[docker-compose.yml](docker-compose.yml)** - Docker Composeä¸»é…ç½®

## ğŸ“‹ éƒ¨ç½²æˆæœ

### âœ… å·²å®Œæˆ
1. **Dockerå®¹å™¨åŒ–ç¯å¢ƒ** - åŸºäºjetson_minicpm_vé•œåƒ
2. **PyTorch CUDAæ”¯æŒ** - PyTorch 2.4.0 + CUDA 12.6
3. **llama.cppé›†æˆ** - æ”¯æŒGPUåŠ é€Ÿæ¨ç†
4. **Q4_K_Mæ¨¡å‹ä¸‹è½½** - 18GBé‡åŒ–æ¨¡å‹ï¼ˆæˆåŠŸï¼‰
5. **ä¸€é”®éƒ¨ç½²è„šæœ¬** - è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹
6. **å®Œæ•´æ–‡æ¡£ä½“ç³»** - ä»å¿«é€Ÿå¼€å§‹åˆ°è¯¦ç»†æŒ‡å—

### ğŸ“Š ç³»ç»Ÿé…ç½®
- **ç¡¬ä»¶å¹³å°**: NVIDIA Jetson AGX Orin 64GB
- **æ“ä½œç³»ç»Ÿ**: Ubuntu, Linux 5.15.148-tegra
- **GPU**: Ampereæ¶æ„, 2048 CUDAæ ¸å¿ƒ, Compute Capability 8.7
- **å†…å­˜é…ç½®**: 64GBç‰©ç†å†…å­˜ + äº¤æ¢ç©ºé—´
- **å­˜å‚¨é…ç½®**: æ¨¡å‹18GB + ç¼“å­˜ç©ºé—´

### ğŸ¯ æ€§èƒ½é¢„æœŸ
- **æ¨ç†é€Ÿåº¦**: 3-5 tokens/ç§’
- **å†…å­˜å ç”¨**: 20-25GB (å«æ¨¡å‹)
- **ä¸Šä¸‹æ–‡é•¿åº¦**: æœ€å¤§8192 tokens
- **GPUå±‚æ•°**: 35å±‚ (æ¨èé…ç½®)

## ğŸš€ ä½¿ç”¨æµç¨‹

### 1. å¿«é€Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰
```bash
cd /data/deepresearch
./start.sh                    # å¯åŠ¨åŸºç¡€ç¯å¢ƒ
./start-server.sh            # å¯åŠ¨APIæœåŠ¡å™¨ï¼ˆå¯é€‰ï¼‰
```

### 2. æ‰‹åŠ¨æ“ä½œ
```bash
# å¯åŠ¨å®¹å™¨
docker-compose up -d jetson-deepresearch

# è¿›å…¥å®¹å™¨
docker-compose exec jetson-deepresearch bash

# æµ‹è¯•ç¯å¢ƒ
./test_deepresearch.sh

# ç¯å¢ƒè¯Šæ–­
./diagnose.sh
```

### 3. APIä½¿ç”¨
```bash
# è·å–æ¨¡å‹åˆ—è¡¨
curl http://localhost:8080/v1/models

# æ–‡æœ¬ç”Ÿæˆ
curl -X POST http://localhost:8080/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf",
    "prompt": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
/data/deepresearch/
â”œâ”€â”€ docker-compose.yml          # Docker Composeé…ç½®
â”œâ”€â”€ start.sh                    # ä¸€é”®å¯åŠ¨åŸºç¡€ç¯å¢ƒ
â”œâ”€â”€ start-server.sh            # ä¸€é”®å¯åŠ¨APIæœåŠ¡å™¨
â”œâ”€â”€ test_deepresearch.sh       # ç¯å¢ƒæµ‹è¯•è„šæœ¬
â”œâ”€â”€ diagnose.sh                # ç¯å¢ƒè¯Šæ–­è„šæœ¬
â”œâ”€â”€ INSTALLATION_GUIDE.md      # å®Œæ•´å®‰è£…æŒ‡å—
â”œâ”€â”€ README_DOCKER.md           # Dockerè¯¦ç»†æ–‡æ¡£
â”œâ”€â”€ QUICK_START.md             # å¿«é€Ÿä¸Šæ‰‹æŒ‡å—
â””â”€â”€ PROJECT_SUMMARY.md         # é¡¹ç›®æ€»ç»“ï¼ˆæœ¬æ–‡ä»¶ï¼‰

/data/sensor-voice/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf  # 18GB Q4_K_Mæ¨¡å‹
â”œâ”€â”€ llama.cpp/                 # å·²ç¼–è¯‘çš„llama.cppå·¥å…·
â””â”€â”€ DeepResearch/              # é¡¹ç›®ä»£ç ç›®å½•
```

## ğŸ”§ å…³é”®å‘½ä»¤æ±‡æ€»

### ç¯å¢ƒç®¡ç†
```bash
# å¯åŠ¨ç¯å¢ƒ
docker-compose up -d

# åœæ­¢ç¯å¢ƒ
docker-compose down

# æŸ¥çœ‹çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# è¿›å…¥å®¹å™¨
docker-compose exec jetson-deepresearch bash
```

### æ¨¡å‹æµ‹è¯•
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export LD_LIBRARY_PATH=/workspace/llama.cpp/build/bin:$LD_LIBRARY_PATH

# æµ‹è¯•æ¨¡å‹
/workspace/llama.cpp/build/bin/llama-cli \
  --model /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf \
  --ctx-size 1024 \
  --n-gpu-layers 35 \
  --threads 8 \
  --predict 50 \
  --prompt "æµ‹è¯•" \
  --temp 0.7
```

### æœåŠ¡å™¨æ¨¡å¼
```bash
# å¯åŠ¨æœåŠ¡å™¨
/workspace/llama.cpp/build/bin/llama-server \
  --model /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf \
  --ctx-size 8192 \
  --n-gpu-layers 35 \
  --port 8080 \
  --host 0.0.0.0
```

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

### 1. é¡¹ç›®é›†æˆ
```bash
cd /data/sensor-voice/DeepResearch
# è¿è¡Œä½ çš„å…·ä½“é¡¹ç›®ä»£ç 
python inference/run_multi_react.py --help
```

### 2. æ€§èƒ½ä¼˜åŒ–
- è°ƒæ•´GPUå±‚æ•°ï¼ˆæ ¹æ®å†…å­˜æƒ…å†µï¼‰
- ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
- è°ƒæ•´ä¸Šä¸‹æ–‡é•¿åº¦

### 3. ç”Ÿäº§éƒ¨ç½²
- é…ç½®è´Ÿè½½å‡è¡¡
- è®¾ç½®ç›‘æ§å‘Šè­¦
- å®ç°è‡ªåŠ¨é‡å¯

## ğŸ” æ•…éšœæ’æŸ¥

### å¿«é€Ÿè¯Šæ–­
```bash
./diagnose.sh        # è¿è¡Œå®Œæ•´è¯Šæ–­
```

### å¸¸è§é—®é¢˜
1. **CUDAä¸å¯ç”¨** â†’ æ£€æŸ¥nvidia-smiå’ŒDocker runtime
2. **æ¨¡å‹åŠ è½½å¤±è´¥** â†’ éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
3. **å†…å­˜ä¸è¶³** â†’ å‡å°‘GPUå±‚æ•°æˆ–ä¸Šä¸‹æ–‡é•¿åº¦
4. **å®¹å™¨å¯åŠ¨å¤±è´¥** â†’ æ£€æŸ¥Dockeræ—¥å¿—å’Œèµ„æº

## ğŸ“ æ”¯æŒ

### è‡ªæˆ‘æ’æŸ¥
1. è¿è¡Œè¯Šæ–­è„šæœ¬: `./diagnose.sh`
2. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—: `docker-compose logs -f`
3. éªŒè¯GPUçŠ¶æ€: `nvidia-smi`
4. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: `ls -lh /data/sensor-voice/models/`

### ç¯å¢ƒä¿¡æ¯
- **éƒ¨ç½²æ—¶é—´**: $(date)
- **Jetsonå‹å·**: AGX Orin 64GB
- **ç³»ç»Ÿç‰ˆæœ¬**: Ubuntu, Linux 5.15.148-tegra
- **CUDAç‰ˆæœ¬**: 12.6
- **PyTorchç‰ˆæœ¬**: 2.4.0
- **æ¨¡å‹ç‰ˆæœ¬**: Q4_K_Mé‡åŒ–ï¼ˆ18GBï¼‰

---

ğŸ‰ **æ­å–œï¼ä½ ç°åœ¨æ‹¥æœ‰äº†ä¸€ä¸ªå®Œæ•´ã€æ ‡å‡†åŒ–çš„Jetson AGX Orin DeepResearchéƒ¨ç½²ç¯å¢ƒã€‚**

è¿™ä¸ªç¯å¢ƒå…·å¤‡ï¼š
- âœ… ä¸€é”®éƒ¨ç½²èƒ½åŠ›
- âœ… å®Œæ•´çš„æ–‡æ¡£æ”¯æŒ
- âœ… è‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·
- âœ… æ•…éšœè¯Šæ–­åŠŸèƒ½
- âœ… APIæœåŠ¡æ”¯æŒ

ä½ å¯ä»¥å°†è¿™ä¸ªéƒ¨ç½²æ–¹æ¡ˆåº”ç”¨åˆ°ä»»ä½•Jetson AGX Orinç¯å¢ƒä¸­ï¼éœ€è¦æˆ‘å¸®ä½ æµ‹è¯•å…·ä½“çš„åŠŸèƒ½æˆ–è€…ä¼˜åŒ–æ€§èƒ½å—ï¼Ÿ

---

*æœ€åæ›´æ–°: $(date)*