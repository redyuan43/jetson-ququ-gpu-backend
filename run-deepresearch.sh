#!/bin/bash

# Jetson DeepResearch ä¸€é”®è¿è¡Œè„šæœ¬
# åŒ…å«æ‰€æœ‰å¤æ‚çš„ç¯å¢ƒé…ç½®å’Œè¿è¡ŒæŒ‡ä»¤

set -e  # é‡åˆ°é”™è¯¯æ—¶é€€å‡º

echo "ğŸš€ Jetson DeepResearch ä¸€é”®è¿è¡Œè„šæœ¬"
echo "=================================="

# æ·»åŠ Docker Composeåˆ°PATH
export PATH=$PATH:~/.local/bin

# æ£€æŸ¥Docker Compose
echo "ğŸ“¦ æ£€æŸ¥Docker Compose..."
if ! command -v docker-compose > /dev/null; then
    echo "âŒ Docker Composeæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…"
    exit 1
fi

echo "âœ… Docker Composeå·²å®‰è£…: $(docker-compose --version)"

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
echo "ğŸ§  æ£€æŸ¥æ¨¡å‹æ–‡ä»¶..."
MODEL_PATH="/data/sensor-voice/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf"
if [ ! -f "$MODEL_PATH" ]; then
    echo "âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: $MODEL_PATH"
    echo "è¯·å…ˆä¸‹è½½æ¨¡å‹:"
    echo "  huggingface-cli download bartowski/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-GGUF Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf --local-dir /data/sensor-voice/models"
    exit 1
fi

echo "âœ… æ¨¡å‹æ–‡ä»¶å­˜åœ¨: $(ls -lh $MODEL_PATH | awk '{print $5}')"

# æ£€æŸ¥llama.cpp
echo "âš™ï¸  æ£€æŸ¥llama.cpp..."
LLAMA_PATH="/data/sensor-voice/sensor-voice/llama.cpp/build/bin/llama-cli"
if [ ! -f "$LLAMA_PATH" ]; then
    echo "âŒ llama.cppæœªç¼–è¯‘ï¼Œè¯·å…ˆç¼–è¯‘:"
    echo "  cd /data/sensor-voice/sensor-voice/llama.cpp && make -j8 LLAMA_CUDA=1"
    exit 1
fi

echo "âœ… llama.cppå¯ç”¨"

# æ£€æŸ¥DeepResearché¡¹ç›®
echo "ğŸ“ æ£€æŸ¥DeepResearché¡¹ç›®..."
DEEPRESEARCH_PATH="/data/deepresearch/DeepResearch"
if [ ! -d "$DEEPRESEARCH_PATH" ]; then
    echo "âŒ DeepResearché¡¹ç›®ä¸å­˜åœ¨: $DEEPRESEARCH_PATH"
    exit 1
fi

echo "âœ… DeepResearché¡¹ç›®å­˜åœ¨"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p /data/deepresearch/outputs

# åœæ­¢ç°æœ‰å®¹å™¨
echo "ğŸ›‘ åœæ­¢ç°æœ‰å®¹å™¨..."
docker-compose down 2>/dev/null || true

# æ¸…ç†æ—§å®¹å™¨
echo "ğŸ§¹ æ¸…ç†æ—§å®¹å™¨..."
docker container prune -f >/dev/null 2>&1 || true

# å¯åŠ¨åŸºç¡€ç¯å¢ƒ
echo "ğŸ”¥ å¯åŠ¨DeepResearchåŸºç¡€ç¯å¢ƒ..."
docker-compose up -d jetson-deepresearch

echo "â³ ç­‰å¾…å®¹å™¨å¯åŠ¨..."
sleep 10

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
echo "ğŸ” æ£€æŸ¥å®¹å™¨çŠ¶æ€..."
if docker-compose ps | grep -q "Up.*jetson-deepresearch"; then
    echo "âœ… åŸºç¡€å®¹å™¨å¯åŠ¨æˆåŠŸ"
else
    echo "âŒ åŸºç¡€å®¹å™¨å¯åŠ¨å¤±è´¥"
    docker-compose logs jetson-deepresearch
    exit 1
fi

# æµ‹è¯•ç¯å¢ƒ
echo "ğŸ§ª æµ‹è¯•ç¯å¢ƒ..."
docker-compose exec -T jetson-deepresearch bash -c "
    echo 'Testing PyTorch CUDA...' &&
    python3 -c 'import torch; print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")' &&
    echo 'Testing llama.cpp...' &&
    export LD_LIBRARY_PATH=/workspace/llama.cpp/build/bin:\$LD_LIBRARY_PATH &&
    /workspace/llama.cpp/build/bin/llama-cli --version
"

echo ""
echo "ğŸ¯ è¿è¡Œé€‰é¡¹:"
echo "1. è¿è¡ŒåŸºç¡€æµ‹è¯• (æ¨è)"
echo "2. è¿è¡Œllama.cppæœåŠ¡å™¨"
echo "3. è¿è¡ŒDeepResearchæ¨ç†"
echo "4. è¿›å…¥äº¤äº’æ¨¡å¼"
echo ""

read -p "è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ (1-4): " choice

case $choice in
    1)
        echo "ğŸ§ª è¿è¡ŒåŸºç¡€æµ‹è¯•..."
        docker-compose exec jetson-deepresearch bash -c "
            export LD_LIBRARY_PATH=/workspace/llama.cpp/build/bin:\$LD_LIBRARY_PATH &&
            echo 'Testing model loading...' &&
            /workspace/llama.cpp/build/bin/llama-cli \
                --model /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf \
                --ctx-size 512 \
                --n-gpu-layers 35 \
                --threads 8 \
                --predict 20 \
                --prompt 'ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹é˜¿é‡Œå·´å·´çš„WebAgentæŠ€æœ¯' \
                --temp 0.7 \
                --no-display-prompt
        "
        ;;

    2)
        echo "ğŸŒ å¯åŠ¨llama.cppæœåŠ¡å™¨..."
        docker-compose --profile server up -d llama-server
        echo "âœ… llama.cppæœåŠ¡å™¨å·²å¯åŠ¨"
        echo "ğŸ“¡ APIåœ°å€: http://localhost:8080"
        echo "ğŸ“Š æŸ¥çœ‹æ—¥å¿—: docker-compose logs -f llama-server"
        ;;

    3)
        echo "ğŸ§  è¿è¡ŒDeepResearchæ¨ç†..."
        docker-compose --profile inference up deepresearch-runner
        echo "âœ… DeepResearchæ¨ç†å®Œæˆ"
        echo "ğŸ“Š æŸ¥çœ‹è¾“å‡º: ls -la /data/deepresearch/outputs/"
        ;;

    4)
        echo "ğŸ”§ è¿›å…¥äº¤äº’æ¨¡å¼..."
        echo "å¯ç”¨çš„å‘½ä»¤:"
        echo "  export LD_LIBRARY_PATH=/workspace/llama.cpp/build/bin:\$LD_LIBRARY_PATH"
        echo "  /workspace/llama.cpp/build/bin/llama-cli --model /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf --help"
        echo "  python3 /workspace/DeepResearch/inference/run_multi_react.py --help"
        echo ""
        echo "æ­£åœ¨è¿›å…¥å®¹å™¨..."
        docker-compose exec jetson-deepresearch bash
        ;;

    *)
        echo "âŒ æ— æ•ˆé€‰æ‹©"
        exit 1
        ;;
esac

echo ""
echo "ğŸ‰ è¿è¡Œå®Œæˆï¼"
echo "ğŸ“Š æŸ¥çœ‹å®¹å™¨çŠ¶æ€: docker-compose ps"
echo "ğŸ“‹ æŸ¥çœ‹æ—¥å¿—: docker-compose logs -f"
echo "ğŸ§ª æ›´å¤šæµ‹è¯•: ./test_deepresearch.sh"
echo "ğŸ” ç¯å¢ƒè¯Šæ–­: ./diagnose.sh"
echo ""
echo "ğŸ’¡ æç¤º:"
echo "  - æ¨¡å‹æ–‡ä»¶: /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf"
echo "  - llama.cpp: /workspace/llama.cpp/build/bin/llama-cli"
echo "  - DeepResearch: /workspace/DeepResearch/"
echo "  - è¾“å‡ºç›®å½•: /data/deepresearch/outputs/"