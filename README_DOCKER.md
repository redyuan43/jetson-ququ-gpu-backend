# Jetson AGX Orin DeepResearch Docker éƒ¨ç½²æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

ä½¿ç”¨ docker-compose ç®¡ç†ä½ çš„ Jetson DeepResearch ç¯å¢ƒï¼š

### åŸºç¡€å‘½ä»¤

```bash
# å¯åŠ¨ä¸»å®¹å™¨
docker-compose up -d jetson-deepresearch

# è¿›å…¥å®¹å™¨
docker-compose exec jetson-deepresearch bash

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f jetson-deepresearch

# åœæ­¢å®¹å™¨
docker-compose down

# é‡æ–°æ„å»º
docker-compose build
```

### å¯åŠ¨ llama.cpp æœåŠ¡å™¨

```bash
# å¯åŠ¨ llama.cpp æœåŠ¡å™¨ï¼ˆå¸¦ DeepResearch Q4_K_M æ¨¡å‹ï¼‰
docker-compose --profile server up -d llama-server

# æŸ¥çœ‹æœåŠ¡å™¨çŠ¶æ€
docker-compose logs -f llama-server

# æµ‹è¯• API
curl http://localhost:8080/v1/models
```

## ğŸ“‹ ç¯å¢ƒè¯´æ˜

### å®¹å™¨é…ç½®

- **åŸºç¡€é•œåƒ**: `jetson_minicpm_v:latest` (å·²åŒ…å« PyTorch 2.4.0 + CUDA 12.6)
- **GPUæ”¯æŒ**: NVIDIA runtimeï¼Œè‡ªåŠ¨æ£€æµ‹ Jetson GPU
- **å†…å­˜é™åˆ¶**: 64GB (é€‚é… Jetson AGX Orin)
- **CPUé™åˆ¶**: 8æ ¸

### ç›®å½•æ˜ å°„

| ä¸»æœºè·¯å¾„ | å®¹å™¨è·¯å¾„ | ç”¨é€” |
|---------|---------|------|
| `/data/sensor-voice` | `/workspace` | ä¸»å·¥ä½œç›®å½• |
| `/data/sensor-voice/models` | `/workspace/models` | æ¨¡å‹å­˜å‚¨ |
| `/data/sensor-voice/sensor-voice/llama.cpp` | `/workspace/llama.cpp` | llama.cpp å·¥å…· |

### ç¯å¢ƒå˜é‡

- `NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics`
- `CUDA_VISIBLE_DEVICES=0`
- `TORCH_CUDA_ARCH_LIST=8.7` (é€‚é… Orin GPU)
- `LLAMA_CUDA=1` (å¯ç”¨ CUDA æ”¯æŒ)

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºç¡€æ¨ç†æµ‹è¯•

è¿›å…¥å®¹å™¨åï¼š

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export LD_LIBRARY_PATH=/workspace/llama.cpp/build/bin:$LD_LIBRARY_PATH

# æµ‹è¯•æ¨¡å‹
/workspace/llama.cpp/build/bin/llama-cli \
  --model /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf \
  --ctx-size 8192 \
  --n-gpu-layers 35 \
  --threads 12 \
  --temp 0.85 \
  --prompt "è¯·ä»‹ç»é˜¿é‡Œå·´å·´çš„WebAgentæŠ€æœ¯"
```

### 2. è¿è¡ŒæœåŠ¡å™¨æ¨¡å¼

```bash
# å¯åŠ¨æœåŠ¡å™¨
/workspace/llama.cpp/build/bin/llama-server \
  --model /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf \
  --ctx-size 8192 \
  --n-gpu-layers 35 \
  --port 8080 \
  --host 0.0.0.0
```

### 3. è¿è¡Œ DeepResearch é¡¹ç›®

```bash
cd /data/sensor-voice/DeepResearch
# è¿è¡Œä½ çš„æµ‹è¯•ä»£ç 
python inference/run_multi_react.py --help
```

## ğŸ”§ æ•…éšœæ’é™¤

### å®¹å™¨å¯åŠ¨å¤±è´¥

```bash
# æ£€æŸ¥æ—¥å¿—
docker-compose logs jetson-deepresearch

# é‡æ–°åˆ›å»ºå®¹å™¨
docker-compose down
docker-compose up -d
```

### GPU ä¸å¯ç”¨

```bash
# åœ¨å®¹å™¨å†…æ£€æŸ¥
docker-compose exec jetson-deepresearch bash
python3 -c "import torch; print('CUDA:', torch.cuda.is_available())"

# æ£€æŸ¥ NVIDIA runtime
docker info | grep -i nvidia
```

### å†…å­˜ä¸è¶³

```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
docker stats

# è°ƒæ•´å†…å­˜é™åˆ¶ï¼ˆç¼–è¾‘ docker-compose.ymlï¼‰
mem_limit: 32g  # å‡å°åˆ° 32GB
```

## ğŸ“Š æ€§èƒ½é¢„æœŸ

- **æ¨¡å‹å¤§å°**: 18GB (Q4_K_M é‡åŒ–)
- **æ¨ç†é€Ÿåº¦**: 3-5 tokens/ç§’ (Jetson AGX Orin)
- **å†…å­˜å ç”¨**: 20-25GB (å«æ¨¡å‹åŠ è½½)
- **åŠŸè€—**: 30-45W

## ğŸ”„ æ›´æ–°å’Œç»´æŠ¤

```bash
# æ›´æ–°é•œåƒ
docker-compose pull

# æ¸…ç†æ— ç”¨é•œåƒ
docker image prune -a

# å¤‡ä»½æ¨¡å‹
cp /data/sensor-voice/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf /backup/
```

## ğŸ“ æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥å®¹å™¨æ—¥å¿—: `docker-compose logs`
2. éªŒè¯GPUçŠ¶æ€: `nvidia-smi`
3. ç¡®è®¤æ¨¡å‹æ–‡ä»¶: `ls -lh /data/sensor-voice/models/`

## âš¡ ä¸€é”®å¯åŠ¨è„šæœ¬

åˆ›å»º `start.sh`:

```bash
#!/bin/bash
echo "ğŸš€ å¯åŠ¨ Jetson DeepResearch ç¯å¢ƒ..."
docker-compose up -d jetson-deepresearch
echo "âœ… å®¹å™¨å·²å¯åŠ¨ï¼Œä½¿ç”¨ 'docker-compose exec jetson-deepresearch bash' è¿›å…¥"
```

åˆ›å»º `start-server.sh`:

```bash
#!/bin/bash
echo "ğŸš€ å¯åŠ¨ llama.cpp æœåŠ¡å™¨..."
docker-compose --profile server up -d llama-server
echo "âœ… æœåŠ¡å™¨å·²å¯åŠ¨ï¼ŒAPI åœ°å€: http://localhost:8080"
```