# ğŸš€ Jetson DeepResearch å¿«é€Ÿå¯åŠ¨

## 1åˆ†é’Ÿä¸Šæ‰‹

### 1. å¯åŠ¨ç¯å¢ƒ
```bash
# ä¸€é”®å¯åŠ¨åŸºç¡€ç¯å¢ƒ
./start.sh

# è¿›å…¥å®¹å™¨
docker-compose exec jetson-deepresearch bash
```

### 2. æµ‹è¯•æ¨¡å‹
```bash
# åœ¨å®¹å™¨å†…è¿è¡Œæµ‹è¯•
./test_deepresearch.sh
```

### 3. å¯åŠ¨APIæœåŠ¡å™¨
```bash
# ä¸€é”®å¯åŠ¨æœåŠ¡å™¨
./start-server.sh

# æµ‹è¯•API
curl http://localhost:8080/v1/models
```

## ğŸ¯ å¸¸ç”¨å‘½ä»¤

### å®¹å™¨ç®¡ç†
```bash
# å¯åŠ¨
docker-compose up -d

# åœæ­¢
docker-compose down

# é‡å¯
docker-compose restart

# æŸ¥çœ‹çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

### æ¨¡å‹æµ‹è¯•
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export LD_LIBRARY_PATH=/workspace/llama.cpp/build/bin:$LD_LIBRARY_PATH

# ç®€å•æµ‹è¯•
/workspace/llama.cpp/build/bin/llama-cli \
  --model /workspace/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf \
  --ctx-size 1024 \
  --n-gpu-layers 35 \
  --threads 8 \
  --predict 50 \
  --prompt "ä½ å¥½" \
  --temp 0.7
```

### APIè°ƒç”¨
```bash
# è·å–æ¨¡å‹åˆ—è¡¨
curl http://localhost:8080/v1/models

# æ–‡æœ¬ç”Ÿæˆ
curl -X POST http://localhost:8080/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf",
    "prompt": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
/data/deepresearch/
â”œâ”€â”€ docker-compose.yml          # Docker Composeé…ç½®
â”œâ”€â”€ start.sh                    # ä¸€é”®å¯åŠ¨è„šæœ¬
â”œâ”€â”€ start-server.sh            # å¯åŠ¨APIæœåŠ¡å™¨
â”œâ”€â”€ test_deepresearch.sh       # ç¯å¢ƒæµ‹è¯•è„šæœ¬
â”œâ”€â”€ README_DOCKER.md           # è¯¦ç»†æ–‡æ¡£
â””â”€â”€ QUICK_START.md             # å¿«é€ŸæŒ‡å—

/data/sensor-voice/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf  # 18GBæ¨¡å‹
â”œâ”€â”€ llama.cpp/                 # å·²ç¼–è¯‘çš„llama.cpp
â””â”€â”€ DeepResearch/              # é¡¹ç›®ä»£ç 
```

## âš¡ æ€§èƒ½æŒ‡æ ‡

- **æ¨¡å‹å¤§å°**: 18GB (Q4_K_Mé‡åŒ–)
- **å†…å­˜å ç”¨**: 20-25GB
- **æ¨ç†é€Ÿåº¦**: 3-5 tokens/ç§’
- **ä¸Šä¸‹æ–‡é•¿åº¦**: æœ€å¤§8192 tokens
- **GPUå±‚æ•°**: 35å±‚ (æ¨è)

## ğŸ”§ æ•…éšœæ’æŸ¥

### å®¹å™¨æ— æ³•å¯åŠ¨
```bash
# æ£€æŸ¥DockerçŠ¶æ€
sudo systemctl status docker

# æ£€æŸ¥NVIDIA runtime
docker info | grep -i nvidia
```

### GPUä¸å¯ç”¨
```bash
# åœ¨å®¹å™¨å†…æ£€æŸ¥
python3 -c "import torch; print('CUDA:', torch.cuda.is_available())"

# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi
```

### æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -lh /data/sensor-voice/models/Alibaba-NLP_Tongyi-DeepResearch-30B-A3B-Q4_K_M.gguf

# æ£€æŸ¥llama.cpp
ls -la /data/sensor-voice/sensor-voice/llama.cpp/build/bin/
```

## ğŸš€ ä¸‹ä¸€æ­¥

1. **è¿è¡ŒDeepResearché¡¹ç›®**:
   ```bash
   cd /data/sensor-voice/DeepResearch
   python inference/run_multi_react.py --help
   ```

2. **é›†æˆåˆ°åº”ç”¨**: ä½¿ç”¨APIæ¥å£é›†æˆåˆ°ä½ çš„åº”ç”¨ä¸­

3. **æ€§èƒ½ä¼˜åŒ–**: è°ƒæ•´GPUå±‚æ•°å’Œæ‰¹å¤„ç†å¤§å°

## ğŸ“ æ”¯æŒ

æœ‰é—®é¢˜ï¼Ÿæ£€æŸ¥è¿™äº›ï¼š
- æŸ¥çœ‹æ—¥å¿—: `docker-compose logs`
- éªŒè¯GPU: `nvidia-smi`
- æµ‹è¯•ç¯å¢ƒ: `./test_deepresearch.sh`
- æ–‡æ¡£: `README_DOCKER.md`